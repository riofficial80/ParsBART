{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBEQ1pGoCFs",
        "outputId": "2271aa70-b534-4367-a85c-f4a68505b346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt7uGF5xo1Jb",
        "outputId": "c7d461fa-e6b5-4379-9568-7cf2d17d6219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=5f94a8e207082efc962c3e6cf0b0afed435a70c2fec85ee54bdf24e1bbfc8c04\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: xxhash, langdetect, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 langdetect-1.0.9 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ssf4aHTMpD3"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/ParsBART/bart-ir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxIbUV39oKFi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "import re\n",
        "import langdetect\n",
        "import pandas as pd\n",
        "import timeit\n",
        "import torch\n",
        "import numpy as np\n",
        "from data.utils import sentence_permutation, document_rotation\n",
        "from data.utils import token_infilling, token_masking, token_deletion\n",
        "from transformers import BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_row_idx_file = open('/content/drive/MyDrive/ParsBART/cleaned_data/cleaned_row_idx.txt', \"r\", encoding=\"utf-8\")\n",
        "cleaned_row_counter_file = open('/content/drive/MyDrive/ParsBART/cleaned_data/cleaned_row_counter.txt', \"r\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "GubppdjY_kik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skiped_rows = int(cleaned_row_idx_file.read())\n",
        "print(skiped_rows)\n",
        "cleaned_row_idx_file.close()\n",
        "saved_clean_rows = int(cleaned_row_counter_file.read())\n",
        "print(saved_clean_rows)\n",
        "cleaned_row_counter_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6L7rN7xeAcE",
        "outputId": "93e98f10-8960-4ff7-84d6-d74ab88e52b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125844577\n",
            "1800000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ByVLe11oNgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d73a185-395a-4c42-90ef-5770ada1dbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for SLPL/naab contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/SLPL/naab.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        }
      ],
      "source": [
        "max_mem_size = 4\n",
        "sentence_sep_pattern = \"([!\\.\\?⸮؟]+)[ \\n]+\"\n",
        "min_line_per_doc = 4\n",
        "min_word_per_line = 5\n",
        "end_marks = (\"!\",\".\",\"?\",\"⸮\",\"؟\")\n",
        "words_should_be_filtered_out = [\n",
        "    \"برچسب:\",\n",
        "    \"برچسب ها :\",\n",
        "    \"» مطالب و مقالات »\",\n",
        "    \"برچسبها:\",\n",
        "    \"برچسب :\",\n",
        "    \"برچسبها :\",\n",
        "    \"برچسب ها:\",\n",
        "    \"http://\",\n",
        "    \"حقوق این وب سایت محفوظ\",\n",
        "    \"فراموشی رمز ورود\",\n",
        "    \"https://\",\n",
        "    \"ورود به سایت\",\n",
        "    \"... ادامه خبر\",\n",
        "    \"بیشتر بدانید ...\",\n",
        "    \"...\",\n",
        "    \"کلید واژه ها:\",\n",
        "    \"کلید واژه ها :\",\n",
        "    \"کلید واژهها:\",\n",
        "    \"archive :\",\n",
        "    \"archives :\",\n",
        "    \"archive:\",\n",
        "    \"archives:\",\n",
        "    \"Archive :\",\n",
        "    \"Archives :\",\n",
        "    \"این مطلب را به اشتراک بگذارید\",\n",
        "    \"کلیه حقوق مادی و معنوی\",\n",
        "    \"هیچ نظری هنوز ثبت نشده است\",\n",
        "    \"گر قبلا در بیان ثبت نام کرده اید می توانید ابتدا وارد شوید\",\n",
        "    \"پست الکترونيک شما میتوانید از این تگهای html استفاده کنید\",\n",
        "    \"پست الکترونیک برای عموم قابل مشاهده باشد اخطار!\",\n",
        "    \"Archive:\",\n",
        "    \"Archives:\",\n",
        "    \"(نظر، انتقاد، پیشنهاد ...)\",\n",
        "    \"...\",\n",
        "    \". . .\"\n",
        "]\n",
        "\n",
        "batch_num = 0\n",
        "map_batch_size = 5000\n",
        "map_batch_num = 0\n",
        "duplicate_set = set()\n",
        "filter_batch_size = 1000\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    pattern = re.compile(sentence_sep_pattern)\n",
        "    text = pattern.sub(r'\\1\\n', text)\n",
        "    sentences = [sentence.replace('\\n', ' ').strip() for sentence in text.split('\\n') if sentence.strip()]\n",
        "    if len(sentences) < min_line_per_doc:\n",
        "        return None\n",
        "\n",
        "    final_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence_hash = hash(sentence)\n",
        "        if sentence_hash in duplicate_set:\n",
        "            continue\n",
        "        else:\n",
        "            duplicate_set.add(sentence_hash)\n",
        "        if len(sentence.split()) < min_word_per_line:\n",
        "            continue\n",
        "        if not sentence.endswith(end_marks):\n",
        "            continue\n",
        "        if \"javascript\" in sentence:\n",
        "            continue\n",
        "        if \"lorem ipsum\" in sentence:\n",
        "            continue\n",
        "        if \"{\" in sentence:\n",
        "            continue\n",
        "        remove = False\n",
        "        for filter_word in words_should_be_filtered_out:\n",
        "            if filter_word in sentence:\n",
        "                remove = True\n",
        "                break\n",
        "        if remove:\n",
        "            continue\n",
        "        final_sentences.append(sentence)\n",
        "    if len(final_sentences) < min_line_per_doc:\n",
        "        return None\n",
        "    cleaned = \" \".join(final_sentences).replace('\\n',\" \")\n",
        "    try:\n",
        "      if langdetect.detect(cleaned) != \"fa\":\n",
        "          return None\n",
        "    except:\n",
        "      return None\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "train_streaming_dataset = load_dataset(\n",
        "    \"SLPL/naab\", split=\"train\", streaming=True\n",
        ").skip(skiped_rows).with_format(type=\"torch\")\n",
        "\n",
        "\n",
        "def add_clean_text(example):\n",
        "    example[\"cleaned_text\"] = []\n",
        "    example[\"idx\"] = []\n",
        "    global map_batch_num\n",
        "    for counter, text in enumerate(example['text']):\n",
        "        cleaned_text = clean_text(text)\n",
        "        if cleaned_text is None:\n",
        "            example[\"cleaned_text\"].append(\"None\")\n",
        "            example[\"idx\"].append(map_batch_num * map_batch_size + counter + skiped_rows)\n",
        "        else:\n",
        "            example[\"cleaned_text\"].append(cleaned_text)\n",
        "            example[\"idx\"].append(map_batch_num * map_batch_size + counter + skiped_rows)\n",
        "    map_batch_num += 1\n",
        "    return example\n",
        "\n",
        "def filter_cleaned_text(example):\n",
        "    filter_array=[]\n",
        "    for counter, cleaned_text in enumerate(example['cleaned_text']):\n",
        "        if cleaned_text == \"None\":\n",
        "          filter_array.append(False)\n",
        "        else:\n",
        "          filter_array.append(True)\n",
        "    return filter_array\n",
        "\n",
        "\n",
        "output = train_streaming_dataset.map(add_clean_text, batched=True, batch_size=map_batch_size)\n",
        "output_filtered = output.filter(filter_cleaned_text, batched=True, batch_size=filter_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPSFlA8ZGhv_"
      },
      "outputs": [],
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/ParsBART/bart-ir/tokenizer_bart_ir\")\n",
        "MAX_POSITION_EMBEDDINGS = 256\n",
        "perturbations = [\n",
        "    document_rotation,\n",
        "    sentence_permutation,\n",
        "    token_infilling,\n",
        "    token_masking,\n",
        "    token_deletion,\n",
        "]\n",
        "\n",
        "perturbations_text_domain = [\n",
        "    document_rotation,\n",
        "    sentence_permutation,\n",
        "]\n",
        "\n",
        "perturbations_token_domain = [\n",
        "    token_infilling,\n",
        "    token_masking,\n",
        "    token_deletion,\n",
        "]\n",
        "\n",
        "def collate_fn(cleaned_text):\n",
        "\n",
        "    perturbation_function = random.choice(perturbations)\n",
        "    if perturbation_function in perturbations_text_domain:\n",
        "        # need to truncate the text to 256 tokens\n",
        "        t_text = tokenizer(cleaned_text, truncation=True, max_length=MAX_POSITION_EMBEDDINGS)\n",
        "        text_truncated = tokenizer.decode(t_text[\"input_ids\"], skip_special_tokens=True)\n",
        "        perturbed_text = perturbation_function(text_truncated)\n",
        "    else:\n",
        "        original_input_ids = tokenizer(\n",
        "            cleaned_text, return_tensors=\"pt\", truncation=True, max_length=MAX_POSITION_EMBEDDINGS\n",
        "        )[\"input_ids\"][0]\n",
        "        perturbed_input_ids = perturbation_function(\n",
        "                tokenized_sequence=original_input_ids,\n",
        "                mask_token_id=tokenizer.mask_token_id,\n",
        "                mask_probability=0.15,\n",
        "                list_special_tokens=tokenizer.all_special_ids,\n",
        "            )\n",
        "        perturbed_text = tokenizer.decode(perturbed_input_ids)\n",
        "\n",
        "    return perturbed_text, perturbation_function.__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oidP8Eo_pTWC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "total_size = 0\n",
        "data={'idx':[],'text':[], 'perturbed_text':[], 'perturbation_function':[]}\n",
        "for counter, batch_example in tqdm(enumerate(output_filtered)):\n",
        "  perturbed_text, perturbation_function = collate_fn(batch_example['cleaned_text'])\n",
        "  data['idx'].append(batch_example['idx'].item())\n",
        "  data['text'].append(batch_example['cleaned_text'])\n",
        "  data['perturbed_text'].append(perturbed_text)\n",
        "  data['perturbation_function'].append(perturbation_function)\n",
        "  total_size += len(perturbed_text.encode('utf-8'))\n",
        "\n",
        "  if (counter + 1) % 100000 == 0:\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "    df.to_csv(f'/content/drive/MyDrive/ParsBART/cleaned_data/df{(counter + 1 + saved_clean_rows) / 100000}.csv', index=False)\n",
        "\n",
        "    df = df.iloc[0:0]\n",
        "    data['idx'].clear()\n",
        "    data['text'].clear()\n",
        "    data['perturbed_text'].clear()\n",
        "    data['perturbation_function'].clear()\n",
        "    cleaned_row_idx_file = open('/content/drive/MyDrive/ParsBART/cleaned_data/cleaned_row_idx.txt', \"w\", encoding=\"utf-8\")\n",
        "    cleaned_row_idx_file.write(str(batch_example['idx'].item() + 1))\n",
        "    cleaned_row_idx_file.close()\n",
        "    cleaned_row_counter_file = open('/content/drive/MyDrive/ParsBART/cleaned_data/cleaned_row_counter.txt', \"w\", encoding=\"utf-8\")\n",
        "    cleaned_row_counter_file.write(str(counter + 1 + saved_clean_rows))\n",
        "    cleaned_row_counter_file.close()\n",
        "    cleaning_log = open('/content/drive/MyDrive/ParsBART/cleaned_data/cleaning_log.txt', \"a\", encoding=\"utf-8\")\n",
        "    cleaning_log.write(f\"Cleaned documetns: {counter + 1 + saved_clean_rows} from: {batch_example['idx'].item() + 1} rows ---- Total size: {total_size/(1024*1024)} MB/n\")\n",
        "    cleaning_log.close()\n",
        "    print(f\"Cleaned documetns: {counter + 1 + saved_clean_rows} from: {batch_example['idx'].item() + 1} rows ---- Total size: {total_size/(1024*1024)} MB\")\n",
        "\n",
        "  if total_size > 5.1 * 1024 * 1024 * 1024:\n",
        "    print(f\"out of total_size range!!!!!!!!!! ----  Cleaned documetns: {counter + 1 + saved_clean_rows} from: {batch_example['idx'].item() + 1} rows ---- Total size: {total_size/(1024*1024)} MB\")\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}