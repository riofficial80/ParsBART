# Persian_BART
6-Layer BART for Persian Language

This repository hosts a 6-layer BART (Bidirectional and Auto-Regressive Transformers) model, pretrained exclusively on Persian text data. The model is designed to handle natural language processing (NLP) tasks with a context length of 256 tokens, providing a robust foundation for applications like text generation, summarization, and translation within the Persian language domain.

ðŸ§  Key Features:
- **Architecture**: BART with 6 encoder and 6 decoder layers.
- **Pretraining**: Persian text corpus for enhanced language understanding.
- **Context Length**: Supports input sequences of up to 256 tokens.
- **Applications**: Summarization, translation, text generation, and more.
