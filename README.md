# Persian_BART
6-Layer BART for Persian Language

This repository hosts a 6-layer BART (Bidirectional and Auto-Regressive Transformers) model, pretrained exclusively on Persian text data (Naab dataset: https://huggingface.co/datasets/SLPL/naab-raw). The model is designed to handle natural language processing (NLP) tasks with a context length of 256 tokens.

ðŸ§  Key Features:
- **Architecture**: BART with 6 encoder and 6 decoder layers.
- **Pretraining**: Persian text corpus for enhanced language understanding.
- **Context Length**: Supports input sequences of up to 256 tokens.
- **Applications**: Summarization, translation, text generation, and more.

## Pretraining
### Data Preprocessing
### Model Architecture
## Fintuning
### NLU tasks
#### Sentiment Analysis
#### Text Classification
### NLG tasks
#### News Summarization
#### Key Generation
