# Persian_Bart
6-Layer BART for Persian Language

This repository hosts a 6-layer BART (Bidirectional and Auto-Regressive Transformers) model, pretrained exclusively on Persian text data. The model is designed to handle natural language processing (NLP) tasks with a context length of 256 tokens, providing a robust foundation for applications like text generation, summarization, and translation within the Persian language domain.
