# Persian_BART
6-Layer BART for Persian Language

This repository hosts a 6-layer BART (Bidirectional and Auto-Regressive Transformers) model, pretrained exclusively on Persian text data (Naab dataset [1]: https://huggingface.co/datasets/SLPL/naab). The model is designed to handle natural language processing (NLP) tasks with a context length of 256 tokens.

ðŸ§  Key Features:
- **Architecture**: BART with 6 encoder and 6 decoder layers.
- **Pretraining**: Persian text corpus for enhanced language understanding.
- **Context Length**: Supports input sequences of up to 256 tokens.
- **Applications**: Summarization, translation, text generation, and more.

## Pretraining
### Data Preprocessing
### Running Pretraining
#### Model Architecture
#### Pretraining Arguments
## Finetuning
### NLU tasks
#### Sentiment Analysis
#### Text Classification
### NLG tasks
#### News Summarization
#### Key Generation


## ðŸ“š References  
[1] **Sabouri, S., Rahmati, E., Gooran, S. and Sameti, H.**, 2024. *naab: A ready-to-use plug-and-play corpus for Farsi.* Journal of Artificial Intelligence, Applications and Innovations, 1(2), pp.1-8 (https://jaiai.iranaiai.ir/article_211486_3e490bce92a8af967a56870c8d200e90.pdf)
